# 📚 Survey

| 名称 | 链接 | 年份 | 涉及的领域 | 代码 | 创新点 | 不足点 |
|:----|:----|:----:|:--------------:|:----:|:------|:------|
| <small>FovEx: Human-Inspired Explanations for Vision Transformers and CNNs</small> | <small>[IJCV](https://arxiv.org/abs/2408.02123)</small> | <small>2025</small> | <small>视觉可解释性</small> | <small>[GitHub](https://github.com/mahadev1995/FovEx)</small> | <small>[概述](#fovex-ijcv-2025)</small> | <small>[概述](#fovex-ijcv-2025)</small> |
| <small>MaIR: A Locality- and Continuity-Preserving Mamba for Image Restoration</small> | <small>[CVPR](https://arxiv.org/abs/2412.20066)</small> | <small>2025</small> | <small>通用图像恢复 </small> | <small>[GitHub](https://github.com/XLearning-SCU/2025-CVPR-MaIR)</small> | <small>[概述](#mair-cvpr-2025)</small> | <small>[概述](#mair-cvpr-2025)</small> |
| <small>Visual-Instructed Degradation Diffusion for All-in-One Image Restoration</small> | <small>[CVPR](https://openaccess.thecvf.com/content/CVPR2025/papers/Luo_Visual-Instructed_Degradation_Diffusion_for_All-in-One_Image_Restoration_CVPR_2025_paper.pdf)</small> | <small>2025</small> | <small>一体化图像恢复</small> | <small>[GitHub](https://github.com/luowyang/Defusion)</small> | <small>[概述](#defusion-cvpr-2025)</small> | <small>[概述](#defusion-cvpr-2025)</small> |
| <small>DarkIR: Robust Low-Light Image Restoration</small> | <small>[CVPR](https://arxiv.org/abs/2412.13443)</small> | <small>2025</small> | <small>低照度图像恢复</small> | <small>[GitHub](https://github.com/cidautai/DarkIR)</small> | <small>[概述](#darkir-cvpr-2025)</small> | <small>[概述](#darkir-cvpr-2025)</small> |
| <small>FaithDiff: Unleashing Diffusion Priors for Faithful Image Super-resolution</small> | <small>[CVPR](https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_FaithDiff_Unleashing_Diffusion_Priors_for_Faithful_Image_Super-resolution_CVPR_2025_paper.pdf)</small> | <small>2025</small> | <small>图像超分辨率</small> | <small>[GitHub](https://github.com/JyChen9811/FaithDiff)</small> | <small>[概述](#faithdiff-cvpr-2025)</small> | <small>[概述](#faithdiff-cvpr-2025)</small> |
| <small>GEM: A Generalizable Ego-Vision Multimodal World Model for Fine-Grained Ego-Motion, Object Dynamics, and Scene Composition Control</small> | <small>[CVPR](https://arxiv.org/abs/2412.11198)</small> | <small>2024</small> | <small>多模态世界模型<br>RGB+Depth+Pose</small> | <small>[GitHub](https://github.com/vita-epfl/GEM)</small> | <small>[概述](#gem-cvpr-2025)</small> | <small>[概述](#gem-cvpr-2025)</small> |
| <small>ProtoDepth: Unsupervised Continual Depth Completion with Prototypes</small> | <small>[CVPR](https://arxiv.org/abs/2503.12745)</small> | <small>2025</small> | <small>RGB+点云<br>深度补全</small> | <small>[GitHub](https://github.com/patrickqrim/ProtoDepth)</small> | <small>[概述](#protodepth-cvpr-2025)</small> | <small>[概述](#protodepth-cvpr-2025)</small> |
| <small>Dynamic Derivation and Elimination: Audio Visual Segmentation with Enhanced Audio Semantics (DDESeg)</small> | <small>[CVPR](https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Dynamic_Derivation_and_Elimination_Audio_Visual_Segmentation_with_Enhanced_Audio_CVPR_2025_paper.html)</small> | <small>2025</small> | <small>音视频<br>目标分割</small> | <small>[GitHub](https://github.com/YenanLiu/DDESeg)</small> | <small>[概述](#ddeseg-cvpr-2025)</small> | <small>[概述](#ddeseg-cvpr-2025)</small> |
| <small>MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis</small> | <small>[CVPR](https://arxiv.org/abs/2412.15322)</small> | <small>2025</small> | <small>视频→音频<br>视听生成</small> | <small>[GitHub](https://github.com/hkchengrex/MMAudio)</small> | <small>[概述](#mmaudio-cvpr-2025)</small> | <small>[概述](#mmaudio-cvpr-2025)</small> |
| <small>MulFS-CAP: Multimodal Fusion-Supervised Cross-Modality Alignment Perception for Unregistered Infrared-Visible Image Fusion</small> | <small>[TPAMI](https://doi.org/10.1109/TPAMI.2025.3535617)</small> | <small>2025</small> | <small>红外+可见光<br>图像融合</small> | <small>[GitHub](https://github.com/YR0211/MulFS-CAP)</small> | <small>[概述](#mulfs-cap-tpami-2025)</small> | <small>[概述](#mulfs-cap-tpami-2025)</small> |
| <small>Improving the Generalization of Segmentation Foundation Model under Distribution Shift via Weakly Supervised Adaptation</small> | <small>[CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Improving_the_Generalization_of_Segmentation_Foundation_Model_under_Distribution_Shift_CVPR_2024_paper.pdf)</small> | <small>2024</small> | <small>SAM<br>弱监督自训练适配<br>分布偏移鲁棒分割</small>  |                                 <small>[GitHub](https://github.com/zhang-haojie/wesam)</small> | <small>[概述](#improving-sfm-generalization-cvpr-2024)</small> | <small>[概述](#improving-sfm-generalization-cvpr-2024)</small> |
| <small>RobustSAM: Segment Anything Robustly on Degraded Images</small>                                                                 |                                  <small>[CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_RobustSAM_Segment_Anything_Robustly_on_Degraded_Images_CVPR_2024_paper.pdf)</small> | <small>2024</small> | <small>SAM<br>退化图像鲁棒分割</small>              |                                <small>[GitHub](https://github.com/robustsam/RobustSAM)</small> |                    <small>[概述](#robustsam-cvpr-2024)</small> |                    <small>[概述](#robustsam-cvpr-2024)</small> |
| <small>Unsupervised Cumulative Domain Adaptation for Foggy Scene Optical Flow</small>                                                  |                  <small>[CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Unsupervised_Cumulative_Domain_Adaptation_for_Foggy_Scene_Optical_Flow_CVPR_2023_paper.pdf)</small> | <small>2023</small> | <small>光流估计<br>雾天域自适应</small>               |                                <small>[GitHub](https://github.com/hyzhouboy/UCDA-Flow)</small> |      <small>[概述](#ucda-foggy-optical-flow-cvpr-2023)</small> |      <small>[概述](#ucda-foggy-optical-flow-cvpr-2023)</small> |
| <small>Mask DINO: Towards a Unified Transformer-Based Framework for Object Detection and Segmentation</small>                          |              <small>[CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Mask_DINO_Towards_a_Unified_Transformer-Based_Framework_for_Object_Detection_CVPR_2023_paper.pdf)</small> | <small>2023</small> | <small>目标检测 + 分割<br>统一Transformer框架</small> |                             <small>[GitHub](https://github.com/IDEA-Research/MaskDINO)</small> |                    <small>[概述](#mask-dino-cvpr-2023)</small> |                    <small>[概述](#mask-dino-cvpr-2023)</small> |
| <small>DiffusionDet: Diffusion Model for Object Detection</small>                                                                      |                                       <small>[ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_DiffusionDet_Diffusion_Model_for_Object_Detection_ICCV_2023_paper.pdf)</small> | <small>2023</small> | <small>目标检测<br>扩散模型检测器</small>              |                            <small>[GitHub](https://github.com/ShoufaChen/DiffusionDet)</small> |                 <small>[概述](#diffusiondet-iccv-2023)</small> |                 <small>[概述](#diffusiondet-iccv-2023)</small> |
| <small>DiffIR: Efficient Diffusion Model for Image Restoration</small>                                                                 |                                   <small>[ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Xia_DiffIR_Efficient_Diffusion_Model_for_Image_Restoration_ICCV_2023_paper.pdf)</small> | <small>2023</small> | <small>图像复原<br>高效扩散</small>                 |                                   <small>[GitHub](https://github.com/Zj-BinXia/DiffIR)</small> |                       <small>[概述](#diffir-iccv-2023)</small> |                       <small>[概述](#diffir-iccv-2023)</small> |
| <small>BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing</small>                |                                                                                                                          <small>[NeurIPS](https://openreview.net/forum?id=g6We1SwaY9)</small> | <small>2023</small> | <small>主体驱动生成/编辑<br>多模态控制</small>           | <small>[GitHub](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion)</small> |            <small>[概述](#blip-diffusion-neurips-2023)</small> |            <small>[概述](#blip-diffusion-neurips-2023)</small> |
| <small>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</small>                                    |      <small>[CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.html)</small> | <small>2023</small> | <small>主体驱动个性化生成<br>少样本微调</small>           |                                  <small>[GitHub](https://github.com/google/dreambooth)</small> |                   <small>[概述](#dreambooth-cvpr-2023)</small> |                   <small>[概述](#dreambooth-cvpr-2023)</small> |
| <small>FreeU: Free Lunch in Diffusion U-Net</small> |  <small>[CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Si_FreeU_Free_Lunch_in_Diffusion_U-Net_CVPR_2024_paper.pdf)</small> | <small>2024</small> | <small>扩散采样质量提升<br>训练-free U-Net 调控</small> |                                   <small>[GitHub](https://github.com/ChenyangSi/FreeU)</small> |                        <small>[概述](#freeu-cvpr-2024)</small> |                        <small>[概述](#freeu-cvpr-2024)</small> |
| <small>AGLLDiff: Guiding Diffusion Models Towards Unsupervised Training-free Real-world Low-light Image Enhancement</small>            | <small>[arXiv](https://arxiv.org/abs/2407.14900)</small> | <small>2024</small> | <small>低照度图像增强<br>训练-free 扩散引导</small>      |                                   <small>[GitHub](https://github.com/LYL1015/AGLLDiff)</small> |                    <small>[概述](#aglldiff-arxiv-2024)</small> |                    <small>[概述](#aglldiff-arxiv-2024)</small> |


## 专有名词解释（点击跳转）
- [CLIP](#clip)
- [DINO](#dino)
- [SAM](#sam)
- [Segment Anything](#segment-anything)
- [BLIP](#blip)
- [Diffusion Models](#diffusion-models)
- [Foundation Models](#foundation-models)
- [Prompting](#prompting)
- [Fine-tuning](#fine-tuning)
- [Adapters](#adapters)
- [Zero-shot Transfer](#zero-shot-transfer)
- [Vision-Language Models](#vision-language-models)
- [Image Enhancement](#image-enhancement)
- [Detection](#detection)
- [Segmentation](#segmentation)
- [Captioning](#captioning)
- [Benchmark](#benchmark)
- [Real-world Datasets](#real-world-datasets)
- [Multimodal Priors](#multimodal-priors)
- [LiDAR](#lidar)
- [Thermal](#thermal)
- [Uncertainty-aware](#uncertainty-aware)
- [Trustworthy](#trustworthy)
- [Low-light](#low-light)
- [Fog](#fog)
- [Underwater](#underwater)
- [Motion Blur](#motion-blur)
- [Low-resolution](#low-resolution)

---

<a id="clip"></a>
### CLIP
>CLIP (Contrastive Language-Image Pretraining) 是由 OpenAI 提出的多模态模型，通过对海量图像-文本对进行对比学习，将图像和文本映射到同一向量空间。借助这种图文对齐的表示，CLIP 能够在无需专门训练的情况下执行图像分类等任务，只需提供类别的文本名称作为提示即可完成识别。

<a id="dino"></a>
### DINO
> DINO (Self-Distillation with NO labels) 是一种无监督视觉特征学习方法。它通过教师-学生网络架构在没有人工标注的情况下训练视觉Transformer模型，学生网络学习预测动态更新的教师网络输出，从而提取出具有丰富语义信息的特征。DINO 的特征不仅在分类上表现优异，还呈现出自动分割图像语义区域的能力。

<a id="sam"></a>
### SAM
> SAM  (Segment Anything Model)是 Meta AI 提出的通用分割模型，可针对任意图像和目标进行可提示的分割。给定点、框等提示，SAM 能够生成目标的高质量分割掩膜，并支持整图的自动分割。它在超过1.1万张图像和11亿分割掩膜的大规模数据集上训练，在各种分割任务上表现出很强的零样本泛化能力，媲美甚至超越以往的有监督方法。

<a id="segment-anything"></a>
### Segment Anything
> Segment Anything 是 Meta 提出的一个分割新范式，包括任务定义、模型和数据集。该项目引入了“大模型+大数据”的理念：通过在含11亿掩膜的超大数据集上训练一个“可提示”的分割模型，使其能够适应新的图像分布和任务而无需额外训练。Segment Anything 的核心模型即 SAM，在各种下游分割任务中零样本表现出色，推动了视觉基础模型在分割领域的发展。

<a id="blip"></a>
### BLIP
> BLIP（Bootstrapping Language-Image Pre-training）是一种统一的视觉-语言预训练框架，可同时支持图像-文本理解与生成任务。BLIP 通过生成模型产生图像描述并过滤噪声描述，从而更高效地利用弱标注数据。经过预训练后，BLIP 在图像-文本检索、图像描述生成、视觉问答等任务上取得领先性能，并可零样本泛化到视频描述等新场景。

<a id="diffusion-models"></a>
### Diffusion Models
> Diffusion Models扩散模型是一类强大的生成模型，包括DDPM等，通过正向逐步向数据添加噪声、反向学会去噪来生成新数据。模型训练的目标是学会逆转噪声扩散过程，将纯随机噪声逐步还原为逼真的数据样本。这种迭代去噪生成方法已在图像生成等视觉任务中取得最先进效果。

<a id="foundation-models"></a>
### Foundation Models
> 基础模型指在海量多样数据上训练、可适应众多下游任务的大规模模型。典型特征是使用自监督等方式在广泛语料上学习通用表示，再通过少量微调或提示适配不同任务。这类模型训练成本极高，但下游使用高效。

<a id="prompting"></a>
### Prompting
>在人工智能中，提示指通过输入精心设计的指令或示例，引导预训练模型产生期望结果的技术。用户就像给模型提出问题或描述要求，模型则根据提示内容进行推理和生成。

<a id="fine-tuning"></a>
### Fine-tuning
> 将预训练模型在新数据新任务上继续训练，以适应特定下游任务的过程。通常做法是加载在大规模数据上训练好的模型权重，然后用小规模的任务数据对部分或全部层进行再次训练。通过微调，模型能够重用原有知识并针对新任务进行优化，比从头训练更高效，已成为迁移学习的标准手段。

<a id="adapters"></a>
### Adapters
> 一种轻量级的可训练模块，插入到预训练模型的各层中，实现高效参数微调。每个适配器包含远少于原模型的参数，在新任务训练时仅调整这些小模块的权重，而保持原模型大部分参数冻结。这种方法显著减少了微调开销，并允许一个模型通过不同适配器快速切换服务多个任务，在自然语言处理和计算机视觉的跨任务迁移中被广泛采用。

<a id="zero-shot-transfer"></a>
### Zero-shot Transfer
> Zero-shot Transfer（零样本迁移）指模型在未见过目标任务或类别训练样本的情况下，仍能直接泛化完成识别或推理。实现零样本迁移通常依赖辅助信息将已知与未知类别关联起来；CLIP 等多模态模型常以文本作为语义桥梁，从而具备零样本分类能力。

<a id="vision-language-models"></a>
### Vision-Language Models
> Vision-Language Models同时处理图像和文本两种模态的AI模型。它通过联合训练图像编码器和文本编码器，学会将视觉和语言信息对齐，从而能够执行图文检索、图像描述生成、视觉问答等跨模态任务。典型VLM架构包含一个图像特征提取网络和一个文本特征提取网络，二者输出的嵌入在共享空间中对应同一语义，实现“看到图像→生成文字”或“读取文字→找出图像”等功能。

<a id="image-enhancement"></a>
### Image Enhancement
> 通过算法调整图像的亮度、对比度、颜色等以改善图像视觉质量的过程。简单增强操作包括使图像整体变亮或者变暗、增加对比度等。更高级的照片增强软件还提供去噪、锐化、白平衡校正等滤镜，以消除各种缺陷。图像增强能够提升人眼观感和机器视觉算法的输入质量，例如低光照增强可让后续目标检测在夜间环境下更准确。

<a id="detection"></a>
### Detection
> Detection 一种计算机视觉技术，用于在图像或视频中识别并定位出多个感兴趣目标。与仅给出类别标签的图像分类不同，目标检测不仅要判别对象类别，还需用边界框圈出每个对象的位置。例如，在自动驾驶中，检测算法需要找出画面中行人、车辆的位置和种类。常见目标检测模型能够实时处理图像，输出目标边界框及分类结果。

<a id="segmentation"></a>
### Segmentation
> Segmentation（图像分割）：图像分割是将图像划分为多个像素区域的过程，其目的是将图像表示转换为对分析更有意义且易处理的形式。具体而言，分割为图像中每个像素赋予类别标签，使同一类别的像素连成区域、具有相似的视觉特征。分割结果通常以掩膜形式呈现，可实现精细的目标边界提取。根据粒度不同，有语义分割和实例分割等类型，是自动驾驶、医疗影像分析等领域的重要基础任务。

<a id="captioning"></a>
### Captioning
> Captioning（图像描述）：图像描述任务要求为给定图像自动生成自然语言的文字描述。这是一个典型的多模态问题，结合了计算机视觉和自然语言处理：模型通常使用CNN提取图像特征，再由RNN或Transformer解码生成一句话，逐词描述图像内容。

<a id="benchmark"></a>
### Benchmark
> Benchmark (基准)：在机器学习中指标准的数据集、任务或评价指标，用于评测模型性能的统一参照。研究者会将新模型在这些公开基准上测试，以便与前人方法进行公平比较。

<a id="real-world-datasets"></a>
### Real-world Datasets
> Real-world Datasets：真实世界数据集指直接来源于实际应用场景、未经严格挑选清洗的原始数据。与为了研究而整理的干净基准不同，这类数据往往更加多样化且包含噪声，能反映出模型在现实任务中的表现。

<a id="multimodal-priors"></a>
### Multimodal Priors
> Multimodal Priors：在模型推理或生成过程中，融入来自多种数据模态的先验知识或约束。在图像生成任务中，可将分类标签、文本描述、分割掩膜等高层语义信息一同引入，以指导生成结果更符合语义和视觉常识。这些不同模态的先验相当于人为提供的线索，帮助模型减少歧义、生成更真实可信的输出。在融合视觉与语言的应用中，利用多模态先验能显著提高结果的质量和一致性。

<a id="lidar"></a>
### LiDAR
> LiDAR： Light Detection and Ranging, LiDAR是一种主动光学传感技术，通过发射激光脉冲并测量返回时间来计算物体距离，从而生成高精度的三维点云地图。LiDAR 常用于自动驾驶和测绘，能够获取环境的3D结构，即使在弱光或夜间也能探测物体的距离和形状。相比摄像头图像，LiDAR 数据直接提供深度信息且不受光照变化影响，但分辨率较低、数据稀疏。计算机视觉算法常将LiDAR与摄像头结合，综合利用几何和纹理特征以提高感知的可靠性。

<a id="thermal"></a>
### Thermal
> Thermal：利用红外传感器捕获物体表面辐射的热红外线，以生成温度分布图像。与可见光相机不同，热成像相机无需环境光照，在全黑环境或恶劣天气下也能工作，通过物体温度差异将其呈现在图像中。热成像广泛用于夜视安防、工业检测和医疗诊断等领域。在计算机视觉任务中，热图像常与可见光图像结合，以提高在低光、强背光等困难条件下的目标检测和识别能力。例如，夜间行人检测算法可以融合热成像以弥补普通摄像头在黑暗中的不足。

<a id="uncertainty-aware"></a>
### Uncertainty-aware
> Uncertainty-aware：具备不确定性感知的模型能够在给出预测结果的同时，估计该预测的置信度或不确定度。这种能力让AI系统在应用中更可信。模型可以标识出哪些输入情形下自己的判断不可靠，提示人类或上层系统谨慎处理。实现不确定性估计的方法包括贝叶斯神经网络、蒙特卡洛dropout、多模型集成等，它们为每个输出提供概率分布或置信区间，而非单一值。有了不确定性信息，决策者可据此评估AI建议的可信度，在高风险领域尤其重要，可以避免过度信任错误的高置信度输出。

<a id="trustworthy"></a>
### Trustworthy
> Trustworthy：可信赖AI是指在安全性、可靠性、公平性、透明度等方面满足要求，让人类愿意信任和使用的人工智能系统。欧盟高层专家组将可信AI归纳为合法、合乎伦理、健壮三大特性。具体而言，一个可信赖的计算机视觉模型不仅要在技术上鲁棒可靠，还应避免数据和算法偏见导致的不公平决策，并具备一定的可解释性和责任机制来追溯错误来源。

<a id="low-light"></a>
### Low-light
> Low-light：低光照是指光线非常暗弱的成像条件，会导致相机图像信噪比极低。低光环境下获取的图像往往偏暗且伴随大量噪声，细节丢失、颜色失真。这使许多视觉算法在夜晚或黑暗环境中性能骤降，不得不借助红外照明等辅助。为改善低光图像质量，研究人员发展了低光增强算法，试图提升亮度和对比度并抑制噪声。例如，多张曝光堆栈合成、基于Retinex理论的增强、以及深度学习的暗光图像增强网络，都能在一定程度上还原夜间图像的明暗细节，使后续的检测、识别算法在弱光下更加稳健。

<a id="fog"></a>
### Fog
> Fog：大雾天气时，空气中的水汽和微粒会散射光线，使远处物体看起来模糊、对比度显著下降。浓雾环境下，原本清晰的边界和纹理被朦胧的白色遮盖，摄像头视程显著缩短。这对计算机视觉是严峻挑战，特别是自动驾驶中的传感器，需要在雾中可靠探测障碍物。为此，图像去雾（dehazing）技术利用大气物理模型（光在雾中经历的衰减和环境光效应）来恢复图像对比度。典型算法如暗通道先验可以估计场景透射率并重建接近平晴天气的清晰图像。去雾后的图像能见度提升，有助于提高检测、识别算法在有雾条件下的准确性和安全性。

<a id="underwater"></a>
### Underwater
> Underwater：水下成像由于水对光线的强烈吸收和散射而面临特殊困难。首先，不同波长的光在水中衰减程度不同，红光等长波很快被吸收，导致水下照片普遍偏蓝绿色，颜色失真严重。其次，悬浮颗粒对光的散射使得图像对比度降低、清晰度变差，距离稍远的物体就变得朦胧难辨。这些问题给水下目标检测、图像识别带来挑战。为改善水下图像质量，研究者提出了各种水下图像增强方法，包括色偏校正、增大对比度等。通过物理模型校正或学习算法增强，处理后的水下图像能更接近在空气中拍摄的效果，从而提升计算机视觉模型在水下恶劣光学环境下的性能。

<a id="motion-blur"></a>
### Motion Blur
> Motion Blur：由于相机在曝光期间抖动或拍摄对象快速移动，导致图像中出现拖影、模糊的现象。模糊会使物体轮廓拉长、细节消失，严重影响识别和检测的准确性。

<a id="low-resolution"></a>
### Low-resolution
> Low-resolution ：低分辨率图像是指像素分辨率较低、细节尺度很小的图像。例如，从远距离裁切的小人脸、放大倍率不足的监控截图等都属于低分辨率情况。此类图像直接放大会缺乏细节且显著模糊，通过简单插值往往得到的结果边缘锯齿、纹理模糊。低分辨率严重阻碍视觉算法提取有效特征和语义信息。为克服这一问题，超分辨率技术致力于将低清晰度图像重建为高分辨率图像。典型方法利用深度卷积神经网络学习低高分辨率图像映射关系，在放大图像尺寸的同时预测合理的新像素以弥补细节。经过超分辨率处理，图像的纹理和边缘更清晰，从而提高下游如人脸识别、目标检测在低分辨率条件下的准确率。

##  数据集
<a id="fovex-ijcv-2025"></a>
### 📖FovEx: Human-Inspired Explanations for Vision Transformers and CNNs （IJCV 2025）

**数据集：**  
[ImageNet-1K](https://www.image-net.org/challenges/LSVRC/)  
> 用于：在 ImageNet-1K 验证集中随机选取 5000 张**被模型正确分类**的图像，作为 FovEx 评估解释可信度的主要测试集，为 CNN 和 ViT 提供统一的分类场景。  

[MIT1003](https://people.csail.mit.edu/tjudd/WherePeopleLook/)  
> 用于：提供 1003 张图像及对应的人眼注视图，FovEx 用其对比方法生成的解释热力图和**真实人眼凝视模式**的一致性，用来评估解释的类人程度（human alignment）。  

**创新点：**
本文提出了 FovEx，这一结合类人凹视机制与梯度驱动扫视、可同时适用于 CNN 与 ViT 的人类启发式可解释性（XAI）方法，在多项可信度指标与人眼凝视一致性上优于现有方法

**不足点：**
优化目标偏向保留关键信息导致在 DELETE 指标（把解释热力图认为最重要的区域，按重要性从高到低逐步遮住，看模型置信度下降的速度）上表现欠佳且仅在有限数据集与任务上验证，存在泛化性和人群偏置方面的潜在局限。

<a id="mair-cvpr-2025"></a>
### 📖MaIR: A Locality- and Continuity-Preserving Mamba for Image Restoration（CVPR 2025）
**数据集：**  
[DIV2K](https://data.vision.ee.ethz.ch/cvl/DIV2K/)  
> 用于：超分辨率与去噪任务的基础高分辨率训练集之一，为 MaIR 提供多样自然场景图像，并在验证集上评估标准合成退化下的恢复效果。  

[Flickr2K](https://www.kaggle.com/datasets/daehoyang/flickr2k)  
> 用于：与 DIV2K 组成 DF2K 扩充训练集，进一步增加自然场景与纹理多样性，用于超分与去噪训练。  

[WED](https://ivc.uwaterloo.ca/database/WaterlooExploration/)  
> 用于：合成高斯噪声去噪任务的训练数据之一（DFWB 训练集合部分），提升 MaIR 在多种内容和噪声强度下的鲁棒性。  

[BSD400 / BSD68](https://github.com/clausmichele/CBSD68-dataset)  
> 用于：BSD400 参与合成去噪训练；BSD68 作为经典测试集，评估 MaIR 在不同噪声等级下的合成高斯去噪性能。  

[Kodak24](https://r0k.us/graphics/kodak/)  
> 用于：合成噪声去噪的测试集之一，包含 24 张高质量自然图像，用于检验 MaIR 在真实纹理和色彩保持方面的表现。  

[McMaster](https://www4.comp.polyu.edu.hk/~cslzhang/CDM_Dataset.htm)  
> 用于：彩色图像去噪测试集，强调纹理与色彩信息，评估 MaIR 在高频纹理和强颜色区域的恢复能力。  

[Urban100](https://huggingface.co/datasets/eugenesiow/Urban100)  
> 用于：既作为超分辨率任务的测试集（考察城市场景结构和细节恢复），也用于合成噪声去噪评测 MaIR 在复杂几何结构上的表现。  

[SIDD-Medium](https://abdokamel.github.io/sidd/)  
> 用于：真实噪声去噪任务的训练与测试数据集，来自手机拍摄 RAW、 sRGB 图像，用于评估 MaIR 对真实成像噪声的建模与泛化能力。  

[GoPro](https://seungjunnah.github.io/Datasets/gopro.html)  
> 用于：动态场景运动模糊去除任务的主要训练与测试集，评估 MaIR 在复杂相机与物体运动环境下的去模糊能力。  

[HIDE](https://github.com/joanshen0508/HA_deblur)  
> 用于：以人脸和人物为主的去模糊测试集，检验 MaIR 在含人物场景中的细节恢复和感知质量。  

[RESIDE (ITS/OTS/SOTS)](https://sites.google.com/view/reside-dehaze-datasets/reside-standard)  
> 用于：合成图像去雾任务的主要训练与测试数据集，其中 ITS、OTS 用于室内、室外合成雾图训练，SOTS用于标准去雾测试，评估 MaIR 在不同雾密度与场景下的表现。  

[RESIDE-6K](https://gts.ai/dataset-download/reside-6k/)  
> 用于：更大规模、多场景的去雾训练/验证数据集，为 MaIR 提供更丰富的雾天场景和退化分布，进一步提升去雾任务的泛化能力。  


**创新点：**
MaIR 提出在 Mamba 状态空间模型里加入 Nested S-shaped Scanning（用嵌套的 S 形扫描把2D图像转成1D序列时尽量保持局部邻域与空间连续性）+ Sequence Shuffle Attention（对不同扫描展开得到的序列进行打乱并用注意力自适应融合，整合多序列信息、减少单一序列偏差），同时保持图像的局部性和空间连续性，相比以往简单按行或者列展平成 1D 序列的 Mamba 方案，在超分（把低分辨率图片、视频变成更高分辨率）、去噪、去模糊、去雾等 4 大任务、14 个数据集上超过了 40 个已有方法。

**不足点：**
文中没有提到

<a id="defusion-cvpr-2025"></a>
### 📖Visual-Instructed Degradation Diffusion for All-in-One Image Restoration（CVPR 2025）

**数据集：**

[Rain1400](https://xueyangfu.github.io/projects/cvpr2017.html)

> 用于：合成雨条/雨丝去雨（single-image deraining）的经典基准数据集。

[Outdoor-Rain](https://github.com/liruoteng/HeavyRainRemoval)

> 用于：复杂户外重雨场景去雨，评估模型在多种雨型和背景条件下的鲁棒性。

[RESIDE (ITS/OTS/SOTS)](https://sites.google.com/view/reside-dehaze-datasets/reside-standard)

> 用于：单幅图像去雾（dehazing），涵盖室内/室外、合成/真实等多种雾场景，是最常用的去雾基准之一。

[Dense-Haze](https://data.vision.ee.ethz.ch/cvl/ntire19/dense-haze/)

> 用于：高浓度、近似白雾场景的去雾任务，考察在极端雾霾条件下的复原能力。

[Snow100K](https://sites.google.com/view/yunfuliu/desnownet)

> 用于：合成雪花/雪点去雪（desnowing），支持评估不同雪密度与形态下的性能。

[RealSnow](https://github.com/zhuyr97/WGWS-Net)

> 用于：真实采集雪景的去雪任务，检验模型从合成数据泛化到真实场景的能力。

[RainDrop](https://github.com/rui1996/DeRaindrop)

> 用于：去除镜头/玻璃上的雨滴遮挡（raindrop removal），同时恢复被遮挡的背景细节。

[RainDS](https://github.com/Songforrr/RainDS_CCN)

> 用于：同时包含“雨丝 + 雨滴”的混合退化场景，适合评估 all-in-one 去雨模型。

[SIDD](https://abdokamel.github.io/sidd/)

> 用于：真实手机拍摄噪声去噪（denoising），提供成对 noisy/clean 图像。

[GoPro](https://seungjunnah.github.io/Datasets/gopro.html)

> 用于：由相机或物体运动产生的运动模糊去模糊（motion deblurring），经典配对数据集。

[RealBlur](https://cg.postech.ac.kr/research/realblur/)

> 用于：真实拍摄模糊图像的去模糊，相比 GoPro 更贴近真实模糊分布。

[DPDD](https://github.com/Abdullah-Abuolaim/defocus-deblurring-dual-pixel)

> 用于：离焦/景深模糊去模糊（defocus deblurring），基于双像素（dual-pixel）成像。

[LIVE1](https://live.ece.utexas.edu/research/Quality/)

> 用于：图像质量评估及压缩伪影/去噪等任务的小型测试集，常用于客观/主观质量对比。

[NH-HAZE](https://data.vision.ee.ethz.ch/cvl/ntire20/nh-haze/)

> 用于：非均匀雾（non-homogeneous haze）场景去雾，更贴近真实复杂雾分布。

[LHP-Rain](https://github.com/yunguo224/LHP-Rain)

> 用于：大规模真实雨场景去雨（real rain removal），用于评估模型在真实雨环境下的泛化能力。

[WED](https://ivc.uwaterloo.ca/database/WaterlooExploration/)

> 用于：通用自然图像质量评估，以及图像增强/复原算法的测试与对比。

[EUVP](https://irvlab.cs.umn.edu/resources/euvp-dataset)

> 用于：水下图像增强与色彩/对比度恢复（underwater image enhancement）。

**创新点：**
Defusion把all-in-one图像恢复（用一个模型同时处理去噪、去模糊、去雾、低照度增强等不同退化）做成一个视觉指令驱动的退化扩散，不是用模糊的文本prompt，而是构造与不同退化（去噪、去模糊、去雾、低照度等）显式对齐的视觉指令图（用一张长得就像某种退化效果的图，直接当作条件输入去提示扩散模型，让指令和“退化模式”在像素层面一一对应）作为条件去引导扩散模型（Diffusion Model它通过先把数据一步步加噪变乱，再学会一步步去噪把它还原出来来生成图像），对未知退化场景也能统一建模，在多种一体化恢复基准上达到了新的SOTA。
**不足点：**
文中没有提到

<a id="darkir-cvpr-2025"></a>
### 📖DarkIR: Robust Low-Light Image Restoration（CVPR 2024）

**数据集：**  
[LOL-Blur](https://github.com/sczhou/LEDNet#lol-blur-dataset)  
> 用于：DarkIR 的**主训练集**和合成低照+运动模糊场景下的**核心测试基准**，评估同时去噪、去模糊与提亮能力  

[Real-LOLBlur](https://github.com/sczhou/LEDNet#lol-blur-dataset)  
> 用于：从 RealBlur-J 等真实视频中截取的夜景模糊图像，无 GT，作为**真实世界低照+模糊场景的无参考测试集**，检验泛化与视觉效果  

[LOL（LOw-Light paired dataset）](https://daooshee.github.io/BMVC2018website/)  
> 用于：经典**低照度增强配对数据集**（无模糊），在 DarkIR 中作为额外基准，验证网络在纯 LLIE 任务上的恢复质量  

[LOL-v2-Real](https://huggingface.co/datasets/okhater/lolv2-real)  
> 用于：包含 689/100 对真实低照–正常光图像的**配对数据集**，在更复杂真实场景下训练/评估 DarkIR 的低照增强与去噪性能  

[LOL-v2-Synthetic](https://huggingface.co/datasets/okhater/lolv2-synthetic)  
> 用于：900/100 对合成低照–正常光图像，作为补充数据提升 DarkIR 在**合成低照场景**下的泛化与稳定性  

[LSRW](https://github.com/JianghaiSCU/R2RNet#dataset)  
> 用于：由 Nikon 相机与华为手机采集的**真实低照配对数据集**，在不同设备与曝光条件下评估 DarkIR 的跨设备泛化能力

**创新点：**
DarkIR 针对夜景、低照环境下同时存在的噪声、低照度、运动模糊问题，在高效 CNN 上设计新的注意力机制扩展感受野，构建了一个统一的多任务低照度恢复网络，在LOLBlur、LOLv2、Real-LOLBlur等数据集上刷新 SOTA，并在 NTIRE 2025 低照度挑战中获得最佳方法，同时保持参数量和 MAC 数显著低于大多数 Transformer 模型。

**不足点：**
DarkIR 的主要局限在于：虽然通过大量使用depth-wise 卷积显著降低了参数量和 MACs（用来衡量模型的计算量），但作者在 Limitations 中明确指出，这类算子在实际 GPU 上算术强度较低、对硬件不够友好，因此推理时间并不会随着计算量成比例下降；此外，多任务 all-in-one 版本在获得更强泛化能力的同时，在 LOLBlur 等数据集上仍存在约 0.5 dB 的轻微性能损失，说明在统一建模多种低照退化时仍面临精度与泛化、效率之间的折中。

<a id="faithdiff-cvpr-2025"></a>
### 📖FaithDiff: Unleashing Diffusion Priors for Faithful Image Super-resolution（CVPR 2025）

**数据集：**  
[DIV2K](https://data.vision.ee.ethz.ch/cvl/DIV2K/)  
> 用于：合成退化超分的基础训练与验证数据之一，在 DIV2K-Valid 上构造不同退化等级 (D-level) 用于评测模型在标准自然场景上的保真重建能力。  

[LSDIR](https://github.com/ofsoundof/LSDIR)  
> 用于：大规模真实/合成混合恢复数据集，为 FaithDiff 提供多样化高分辨率图像，在 LSDIR-Valid 上合成多种真实感退化以训练和评估对复杂退化的鲁棒性。  

[Flickr2K](https://www.kaggle.com/datasets/daehoyang/flickr2k)  
> 用于：补充自然场景高分辨率图像，与 DIV2K 等一起扩充训练集，提高模型在多类型真实照片上的泛化能力。  

[DIV8K](https://huggingface.co/datasets/yangtao9009/DIV8K)  
> 用于：提供高达 8K 分辨率的图像，帮助模型学习更丰富的高频细节和复杂场景纹理，用于高分辨率超分的训练/验证。  

[FFHQ](https://github.com/NVlabs/ffhq-dataset)  
> 用于：抽取人脸高分辨率图像，专门增强 FaithDiff 在面部细节（皮肤纹理、五官结构等）上的重建能力。  

[RealPhoto60](https://drive.google.com/drive/folders/1yELzm5SvAi9e7kPcO_jPp2XkTs4vK6aR)  
> 用于：由 SUPIR 提供的 60 张真实退化图像集合，在无 GT 的真实场景下评估 FaithDiff 的视觉质量和结构一致性。  

[RealDeg](https://drive.google.com/file/d/1B8BaaMjXJ-1TfcTgE9MrAg8ufvaGkndP/view)  
> 用于：作者新收集的 238 张真实退化图像（老照片、电影剧照、社交媒体图片等），专门用于检验模型在多种未知真实退化类型下的稳健性与泛化表现。  

**创新点：**
FaithDiff 针对“既要好看又要保真”的真实场景超分问题，提出在 latent diffusion（把原始数据用一个编码器“压缩”成一串更短的数字向量后，这些向量所在的空间是latent space，在latent space中做扩散）上加入 特征对齐模块 + 编码器与扩散模型的联合微调，显式对齐退化输入特征与扩散噪声空间，让大模型的先验既能生成细节又不过度幻觉，在多种 SR 基准上对结构保持和视觉质量都明显优于以往扩散式 SR 方法

**不足点：**
文中没有提到

<a id="gem-cvpr-2025"></a>
### 📖GEM: A Generalizable Ego-Vision Multimodal World Model for Fine-Grained Ego-Motion, Object Dynamics, and Scene Composition Control（CVPR 2025）

**数据集：**  
[OpenDV](https://github.com/OpenDriveLab/DriveAGI)  
> 用于：GEM 的核心大规模驾驶视频语料（1700+ 小时多城市、多天气前视视频），作为主要训练数据之一，并在其验证集子集上评估长时序视频生成质量与可控性。  

[nuScenes](https://www.nuscenes.org/)  
> 用于：多传感器自动驾驶数据集（包含 3D 标注与精确轨迹），在 GEM 中既参与训练，又作为带 GT 轨迹的关键评测基准，用于计算 ADE 等控制误差与视频质量指标。  

[DrivingDojo](https://huggingface.co/datasets/Yuqi1997/DrivingDojo)  
> 用于：强调多智能体交互与复杂交通行为的大规模驾驶数据集，为 GEM 提供包含变道、跟车、拥堵等复杂动态的场景，提升模型在高交互场景下的可控生成能力。  

[Honda HDD](https://usa.honda-ri.com/hdd)  
> 用于：包含 100+ 小时真实自然驾驶的视频与车辆 CAN 信号，在 GEM 中作为额外驾驶行为语料，帮助模型学习更贴近日常驾驶风格的世界动态。  

[Honda HAD](https://usa.honda-ri.com/had)  
> 用于：在 HDD 基础上加入人类建议/干预信息的人类驾驶数据集，用于补充带语义引导的驾驶场景，使 GEM 接触到更多样的驾驶意图与操作模式。  

[DoTA](https://github.com/MoonBlvd/Detection-of-Traffic-Anomaly)  
> 用于：交通异常检测数据集，包含大量事故与近事故片段，作为 GEM 训练中的稀有/极端场景补充，使世界模型对异常事件和危险情形的建模更为合理。  

[CarCrashDataset](https://github.com/Cogito2012/CarCrashDataset)  
> 用于：专注真实车祸与正常行驶对比的 dashcam 视频数据集，在 GEM 中进一步丰富安全关键事件样本，提升对碰撞、急刹等极端动态的表达能力。  

[EgoExo4D](https://docs.ego-exo4d-data.org/)  
> 用于：大规模第一/第三人称多视角人类活动数据集，在 GEM 中作为人类活动域的核心数据，用于训练和评估 human-pose 控制与复杂人–物体–场景交互的世界建模能力。  

[self-collected](https://vita-epfl.github.io/GEM.github.io/)  
> 用于：作者从 YouTube 自采集的约 27.4 小时无人机第一视角视频，用于扩展 GEM 到无人机导航域，检验模型在不同高度和视角下的泛化与可控生成能力。  

**创新点：**
提出 GEM 这一统一的自监督多模态世界模型，用单个生成骨干在 4000+ 小时的 RGB 图像、深度、人体姿态和自车轨迹数据上联合建模，利用参考帧 + 稀疏特征 + 控制信号生成未来的 RGB 与深度序列（预测未来看起来什么样，还预测未来空间距离结构怎么变），并通过新的 COM（Control of Object Manipulation） 指标系统定量评估对自车运动、物体动态和场景组合的可控性与跨场景泛化。

**不足点：**
当前模型在超长时序视频上的生成质量和时空一致性仍然有限，而且用于训练的自动伪标注精度受限，从而对控制和泛化能力带来一定约束


<a id="protodepth-cvpr-2025"></a>
### 📖ProtoDepth: Unsupervised Continual Depth Completion with Prototypes（CVPR 2025）
**数据集：**  
[NYU Depth V2](https://cs.nyu.edu/~fergus/datasets/nyu_depth_v2.html)  
> 用于：室内序列的起始数据集 D1，用来预训练室内深度补全模型，以及评估在典型室内场景下的深度补全性能与遗忘程度。  

[VOID](https://github.com/alexklwong/void-dataset)  
> 用于：室内序列中的目标域之一，具有极稀疏深度与强相机运动，作为 室内持续学习序列的后续域，评估 ProtoDepth 在低纹理和大位姿变化场景下的鲁棒性与遗忘情况。  

[ScanNet](https://www.scan-net.org/)  
> 用于：大规模 RGB-D 室内视频数据集，检验方法在跨多种室内场景和传感器配置时的持续适应能力。  

[KITTI](https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion)  
> 用于：室外序列的起始数据集 D1，作为道路场景深度补全预训练基准，并在 KITTI、Waymo、VKITTI 持续学习序列中提供真实自动驾驶场景下的稀疏 LiDAR + RGB 训练与评测。  

[Waymo Open Dataset](https://waymo.com/open/)  
> 用于：室外持续学习序列中的第二个真实自动驾驶数据集，具有更高分辨率和更丰富路况，评估 ProtoDepth 在真实跨域驾驶场景中的适应与遗忘。  

[Virtual KITTI](https://europe.naverlabs.com/proxy-virtual-worlds-vkitti-2/)  
> 用于：合成的 KITTI 场景克隆数据集，在 ProtoDepth 中作为室外序列中的合成目标域，通过添加不同天气和视角变换来模拟 domain shift，用于研究在合成域上的持续学习和遗忘抑制能力。  

[nuScenes](https://www.nuscenes.org/)  
> 用于：未见室外数据集的零样本泛化测试，在训练顺序 KITTI → Waymo → VKITTI 之后，对 nuScenes 进行推理，以评估 ProtoDepth-A 在新城市、新传感器配置下的零样本深度补全泛化性能。  


**创新点：**
将 RGB+稀疏点云深度补全视作原型驱动的持续学习问题，通过跨域共享的深度原型（相当于模型的长期记忆）和域描述符（表示当前场景属于哪种域），在无监督光度重投影框架下实现不同分布间的连续适配，在学习新场景的同时显著缓解传统深度补全模型的遗忘问题。

**不足点：**
方法在实验里能靠已知的域切换点分配原型（模型存的代表性记忆模板），但在真实在线场景中，它还不太会自己判断环境何时变了、该不该新建原型；而且当新旧场景差距很大时，原型匹配可能选错，导致不仅不能完全防遗忘，甚至会拖累性能。

<a id="ddeseg-cvpr-2025"></a>
### 📖Dynamic Derivation and Elimination: Audio Visual Segmentation with Enhanced Audio Semantics（CVPR 2025）
**数据集：**  
[AVS-Object](https://github.com/OpenNLPLab/AVSBench)  
> 用于：DDESeg 的核心基准之一，包含 S4（Single Source）和 MS3（Multi Source）两部分，每段 5 秒视频并在每秒最后一帧提供像素级二值掩码，用于评估“单声源与多声源下是否正确分割出发声区域”。  

[AVS-Semantic](https://github.com/OpenNLPLab/AVSBench)  
> 用于：语义级音视频分割基准，含 12,356 段、71 类音视频片段，每个样本提供语义掩码与音频事件类别标签，用于评估 DDESeg 在“既要找对位置又要分类对类别”的语义 AVS 能力。  

[VPO](https://drive.google.com/file/d/12jq7-Ke09ZPoUI1od44q97DNLrThoHc3/view)  
> 用于：由 COCO 单帧图像与 VGGSound 3 秒音频按类别重配得到的合成 AVS 数据集，包含 VPO-SS 、 VPO-MS 、 VPO-MSMI 三个子集，用来检验 DDESeg 在“单声源、多声源、同类多目标”等更复杂组合场景下的泛化与鲁棒性。  

**创新点：**
从音频本质出发提出 Dynamic Derivation and Elimination 框架：先通过语义重构从混合音频中派生出具有区分性的音源级表示，再用判别特征学习与动态消除模块过滤与画面无关的音源，使真正与视觉目标相关的声音区域得到更精准的匹配，从而在多种 AVS 数据集上显著提升声音引导分割性能。

**不足点：**
文中没有提到


<a id="mmaudio-cvpr-2025"></a>
### 📖MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis（CVPR 2025）
**数据集：**  
[VGGSound](https://www.robots.ox.ac.uk/~vgg/data/vggsound/)  
> 用于：唯一同时包含视频–音频–文本三模态的核心训练与评测集，MMAudio 在其中进行主要的视频→音频训练，并在测试集上评估音频质量、语义对齐度与视听同步性。  

[AudioCaps](https://audiocaps.github.io/)  
> 用于：高质量音频–文本配对数据集，为多模态联合训练提供人工标注描述，并在其测试集上评估文本生成音频能力以及语义一致性。  

[Clotho](https://zenodo.org/record/3490684)  
> 用于：补充 AudioCaps 的另一套音频字幕数据，覆盖更丰富的环境声音和时长分布，用来提升 MMAudio 的音频–语言对齐能力与泛化表现。  

[WavCaps](https://github.com/XinhaoMei/WavCaps)  
> 用于：约 7600 小时的大规模弱标注音频字幕语料，作为主要的大规模音频–文本训练数据源，用于数据扩展和学习更通用的自然声音分布。  

[Greatest Hits](https://andrewowens.com/vis/)  
> 用于：鼓棒敲击物体的视频集合，作为额外的视听同步性评测基准，通过模型无关指标检验 MMAudio 的时间对齐能力。  

[Movie Gen Audio Bench](https://github.com/facebookresearch/MovieGenBench)  
> 用于：合成视频上的出分布评测基准，在无 GT 条件下通过主观评价和 IS、IB-score、CLAP、DeSync 等指标，对比 MMAudio 与 Movie Gen Audio 在复杂生成视频上的音频质量和语义一致性。  

**创新点：**
提出 MMAudio 多模态联合训练框架，将少量视频-音频配对数据与大规模文本-音频数据一起训练，并引入条件同步模块在音频潜变量上做帧级对齐，在流匹配目标下实现 157M 参数规模的高效视频音频生成，在音质、语义对齐和视听同步上都优于现有公开方法，同时还在纯文本生成音频任务上保持竞争力。

**不足点：**
MMAudio 主要面向 Foley 类通用音效（给视频配的音效看着画面生成与画面内容和发生时刻都对齐的声音），对人类语音这类复杂信号支持较弱——在生成说话声时，经常只产生听不清的含糊声音。

### 📖 Improving the Generalization of Segmentation Foundation Model under Distribution Shift (CVPR 2024)

**数据集：**  
[COCO 2017](https://cocodataset.org/#home)  
> 用于：作为“自然/干净域”上的基础数据集，用来评测/对比分割基础模型在常规分布下的分割能力，并作为分布迁移实验的一端。  

[COCO-C](https://github.com/bethgelab/robust-detection-benchmark)  
> 用于：构造“自然 → 破坏/扰动（corruption）”的分布偏移场景，检验模型在噪声、模糊、压缩等退化下的鲁棒性。  

[PASCAL VOC 2012](http://host.robots.ox.ac.uk/pascal/VOC/)  
> 用于：作为自然场景分割基准之一，并在“自然 → 医疗”等跨域设置中充当源域数据集。  

[ISIC](https://challenge.isic-archive.com/)  
> 用于：作为“自然 → 医疗皮肤镜图像”目标域，检验跨域泛化与适配能力。  

[Kvasir-SEG](https://datasets.simula.no/kvasir-seg/)  
> 用于：作为“自然 → 内窥镜/息肉”目标域，检验跨域到医疗内窥镜场景的分割泛化。  

[CAMO](https://sites.google.com/view/camo-dataset)  
> 用于：作为“自然 → 伪装目标（camouflaged）”目标域，测试在难分辨前景/背景下的分割泛化。  

[CHAMELEON](https://github.com/DengPingFan/SINet)  
> 用于：伪装目标分割常用测试集之一，配合 CAMO/COD10K 评测跨域到 camouflaged 场景的性能。  

[COD10K](https://dengpingfan.github.io/pages/COD.html)  
> 用于：更大规模的伪装目标分割数据集，用于系统评测伪装场景下的跨域泛化。  

[OCID](https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/ocid-dataset/)  
> 用于：作为“自然 → 机器人/室内杂乱遮挡”目标域，评测在 clutter/遮挡场景中的分割鲁棒性。  

[OSD（Occluded Object Segmentation Dataset）](https://www.cs.umd.edu/projects/activevision/OSD/)  
> 用于：作为遮挡目标分割目标域之一，评测模型在强遮挡下的分割能力。  

**创新点：**  
- 提出一种“无需源域数据”的弱监督自训练式适配框架：利用弱标注（点/框/粗 mask 等）与伪标签迭代适配分割基础模型，以提升分布偏移下的泛化。  
- 通过正则/约束让提示（prompt）相关的表示更稳定，并采用参数高效微调（如低秩微调）来降低对大模型全量微调的依赖。  

**不足点：**  
- 不同弱监督形式的收益不一致：论文实验中“点提示”适配甚至可能不如不适配，说明方法对监督信号类型较敏感、稳定性仍有限。  


---

### 📖 RobustSAM: Segment Anything Robustly on Degraded Images (CVPR 2024)

**数据集：**  
[Robust-Seg（论文构建的鲁棒分割数据集/基准）](https://robustsam.github.io/)  
> 用于：作为“退化图像分割”的核心训练/评测资源，覆盖多种真实与合成退化（噪声、模糊、低光、雾霾等），用于系统检验 SAM 类基础分割模型的退化鲁棒性。  

[COCO 2017](https://cocodataset.org/#home)  
> 用于：作为基础分割数据来源之一（论文在构建/对比鲁棒性时会使用常规大规模分割数据作支撑/对照）。  

[LVIS](https://www.lvisdataset.org/)  
> 用于：长尾类别更丰富的数据源之一，用于检验鲁棒适配对长尾/复杂类别分割的影响。  

[BDD100K](https://bdd-data.berkeley.edu/)  
> 用于：真实道路场景数据源之一，常用于测试在真实成像条件变化（夜晚/雨雾/运动模糊等）下的鲁棒分割表现。  

（若论文中还使用了特定退化任务数据，如去雾/去模糊对比）  
[RESIDE-SOTS（SOTS）](https://sites.google.com/view/reside-dehaze-datasets/reside-v0)  
> 用于：去雾场景的常用评测集（若论文用于展示鲁棒模块对相关低层视觉任务/退化感知的收益）。  

[GoPro Deblurring Dataset](https://seungjunnah.github.io/Datasets/gopro)  
> 用于：运动去模糊常用基准（同上，若论文用于展示对模糊退化的收益/迁移）。  

**创新点：**  
- 关注“分割基础模型在退化输入上的鲁棒性”这一缺口，构建鲁棒分割数据与评测，并提出轻量化/可插拔的鲁棒增强设计，避免直接全量微调导致的能力退化。  
- 通过对退化分布建模/增强，让 SAM 在低光、雾、噪声、模糊等条件下的分割稳定性显著提升。  

**不足点：**  
- 论文指出直接微调（尤其全量微调）会损害 SAM 的零样本泛化能力，因此方法需要在“增强鲁棒性”和“保持通用性”之间做精细权衡；在某些极端退化/远域细节缺失场景仍可能受限。  


---

### 📖 Unsupervised Cumulative Domain Adaptation for Foggy Scene Optical Flow (CVPR 2023)

**数据集：**  
[KITTI 2015 Optical Flow](https://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow)  
> 用于：作为“干净/晴天源域”光流基准（含稀疏/半稠密标注与真实驾驶场景），在累计式域适配中充当起点。  

[UCDA-Flow 官方实现与数据说明（含 Fog-GOF / DenseFog / Real-Fog World 等实验设置指引）](https://github.com/hyzhouboy/UCDA-Flow)  
> 用于：复现实验的统一入口（论文涉及的 foggy 相关数据/划分/合成方式通常在此给出说明或脚本）。  

[DenseFogFlow（论文提出的 DenseFog 设定来自该方向工作）](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_DenseFogFlow_Dense_Optical_Flow_Estimation_for_Foggy_Scenes_CVPR_2020_paper.pdf)  
> 用于：作为“更强雾化/更贴近真实雾”设定的参考与对比，论文实验中用于验证从合成雾到真实雾的域间差距与改进空间。  

（论文中的 Fog-KITTI2015、Fog-GOF、Real-Fog World 等若未单独提供公开下载页）  
> 用于：它们分别对应“干净→合成雾”“合成雾→真实雾”的不同阶段/目标域，用来验证累计式（cumulative）域适配在真实雾光流上的有效性。  

**创新点：**  
- 提出“累计式无监督域适配”思路：不是一步从 clean 直接到 real fog，而是分阶段（clean→synthetic fog→real fog）逐步迁移，降低域间断崖。  
- 利用雾与深度相关的物理属性，引入与深度/几何相关的适配设计，并通过分布对齐（如相关分布对齐）缩小合成雾与真实雾的差距。  

**不足点：**  
- 论文明确指出：在雾很重且目标很远时（远处运动物体），方法会失败或明显退化，原因包括远距离深度难以准确获取、以及强雾导致细节丢失。  


---

### 📖 Mask DINO: Towards a Unified Transformer-based Framework for Object Detection and Segmentation (CVPR 2023)

**数据集：**  
[COCO 2017](https://cocodataset.org/#home)  
> 用于：统一检测/实例分割/全景分割主基准；验证框架在通用场景下的端到端性能。  

[ADE20K](https://groups.csail.mit.edu/vision/datasets/ADE20K/)  
> 用于：语义分割/场景解析基准；检验方法在密集像素级标注任务上的泛化。  

[Cityscapes](https://www.cityscapes-dataset.com/)  
> 用于：自动驾驶城市场景语义/实例分割基准；检验在街景数据分布下的统一建模能力。  

**创新点：**  
- 以 DETR 系列为核心，将“检测 + 分割（实例/语义/全景）”更统一地纳入同一 Transformer 框架；通过掩码（mask）表征与解码设计提升统一框架的密集预测能力。  

**不足点：**  
- 统一框架往往需要在不同任务头/损失之间做权衡；论文也指出通用统一分割框架整体仍可能落后于某些强专用方法（尤其在特定任务的极致性能上）。  


---

### 📖 DiffusionDet: Diffusion Model for Object Detection (ICCV 2023)

**数据集：**  
[COCO 2017](https://cocodataset.org/#home)  
> 用于：通用目标检测主基准，评测 AP/AP50/AP75 等指标，并做大量消融实验。  

[LVIS](https://www.lvisdataset.org/)  
> 用于：长尾大词表检测/分割基准；检验扩散式检测在更难、更长尾场景下的收益。  

[CrowdHuman](https://www.crowdhuman.org/)  
> 用于：拥挤行人检测场景；检验模型在密集遮挡目标下的检测性能。  

**创新点：**  
- 将目标检测转化为“从噪声框集合逐步去噪/迭代采样”的生成过程：通过扩散反演把随机框逐步收敛到真实目标框集合，形成一种生成式检测范式。  

**不足点：**  
- 论文明确说明：尽管方法简单有效，但还没有结合许多“最强 DETR 系”组件，因此在某些基准上仍落后于当时 SOTA（如 DINO 等）。  


---

### 📖 DiffIR: Efficient Diffusion Model for Image Restoration (ICCV 2023)

**数据集：**  
（图像修复 Inpainting）  
[Places2 / Places](http://places2.csail.mit.edu/)  
> 用于：作为场景图像修复评测（论文表格中常以 Places/Places-Standard 作为修复基准）。  

[CelebA-HQ](https://github.com/tkarras/progressive_growing_of_gans)  
> 用于：人脸高质量图像修复评测（窄/宽 mask 等设置）。  

（超分 SR）  
[DIV2K](https://data.vision.ee.ethz.ch/cvl/DIV2K/)  
> 用于：常用训练/评测基准之一（高质量图像超分）。  

[Flickr2K（常与 DIV2K 组合成 DF2K 的训练集来源之一）](https://github.com/xinntao/BasicSR/blob/master/docs/DatasetPreparation.md)  
> 用于：扩充超分训练数据多样性（许多 SR 工作将 Flickr2K 与 DIV2K 混合作训练）。  

[Set14 / Urban100 / Manga109 等常用 SR 测试集合（聚合下载页）](https://figshare.com/articles/dataset/SR_testsets/25218856)  
> 用于：作为 SR 测试基准，评测 PSNR/LPIPS 等质量与感知指标。  

（去模糊 Deblurring）  
[GoPro Deblurring Dataset](https://seungjunnah.github.io/Datasets/gopro)  
> 用于：作为运动去模糊训练/测试主基准。  

[HIDE Deblurring Dataset](https://github.com/Yaoyi-Li/HIDE)  
> 用于：作为更具挑战的去模糊测试基准之一。  

**创新点：**  
- 面向“扩散模型很强但很慢”的痛点，提出更高效的扩散式恢复框架：通过两阶段/联合优化等设计，让扩散建模能力在更少计算开销下用于修复、超分、去模糊。  

**不足点：**  
- 论文主要验证在三类恢复任务（inpainting / SR / deblurring）上的有效性，对更广泛的退化类型（如去雾、去噪、低光等）是否同样稳定，需要进一步扩展验证。  


---

### 📖 BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing (NeurIPS 2023)

**数据集：**  
（预训练/表征学习数据）  
[MS COCO](https://cocodataset.org/#home)  
> 用于：图文对齐/视觉-语言预训练与下游评测常用基准之一。  

[Visual Genome](https://visualgenome.org/)  
> 用于：更细粒度的区域级标注与图文语义信息，增强多模态表征能力。  

[Conceptual Captions (CC3M)](https://ai.google.com/research/ConceptualCaptions)  
> 用于：大规模弱标注图文数据，支持预训练与泛化。  

[Conceptual 12M (CC12M)](https://github.com/google-research-datasets/conceptual-12m)  
> 用于：更大规模弱标注图文数据，提升长尾与开放域覆盖。  

[SBU Captions](https://www.cs.virginia.edu/~vicente/sbucaptions/)  
> 用于：早期常用弱标注图文数据源之一。  

[LAION-400M](https://laion.ai/blog/laion-400-open-dataset/)  
> 用于：超大规模开放域图文对，增强开放世界生成与编辑能力。  

[Open Images](https://storage.googleapis.com/openimages/web/index.html)  
> 用于：补充大规模真实图片分布与概念覆盖（论文中提到的子集/用法以其实现为准）。  

（个性化/主体控制评测）  
[DreamBench（DreamBooth 官方评测集/协议）](https://dreambooth.github.io/)  
> 用于：评测“主体一致性 + 文本可控性”的标准化基准（也常被后续主体驱动方法沿用）。  

**创新点：**  
- 用预训练的“主体表征”把个性化主体注入到生成/编辑中：相比直接微调整套扩散模型，能更好地兼顾文本可控与主体一致，并支持编辑任务。  

**不足点：**  
- 论文指出在对真实图像做编辑时，仍可能出现编辑文本可控性不足或图像退化等问题（例如难以完全满足复杂编辑指令）。  


---

### 📖 DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation (CVPR 2023)

**数据集：**  
（个性化主体数据）  
[DreamBooth 项目页（含数据/协议与案例）](https://dreambooth.github.io/)  
> 用于：主体驱动生成设置：每个主体仅需少量（通常 3–5 张）图像即可进行个性化生成。  

[DreamBench（官方评测集/协议）](https://dreambooth.github.io/)  
> 用于：标准化评测“主体一致性 + 文本驱动多样性/可控性”，便于不同个性化方法可比。  

（若你需要可复现的下载入口）  
[DreamBooth 官方代码/资源入口](https://github.com/google/dreambooth)  
> 用于：复现实验与评测脚本（数据组织方式以仓库说明为准）。  

**创新点：**  
- 用少量主体图像对文本到图像扩散模型做微调，实现“在多种场景/姿态/风格下仍保持同一主体”的生成；并通过类先验保持（prior preservation）缓解过拟合与语言漂移。  

**不足点：**  
- 论文明确提到：仍可能出现“语言漂移（language drift）”与输出多样性下降，尤其当训练图像太少或提示词设置不当时更明显。  


---

### 📖 FreeU: Free Lunch in Diffusion U-Net (CVPR 2023)

**数据集：**  
（无固定单一数据集依赖）  
> 用于：FreeU 是一种“无需训练/即插即用”的 U-Net 特征重加权策略，论文通常在多种文本提示与多种生成基模型（如 Stable Diffusion 系）上做对比；评价更多依赖生成质量指标与人类偏好实验，而不是绑定单一公开数据集划分。  

**创新点：**  
- 在不重新训练的前提下，通过对 U-Net 的 backbone/skip 分支做简单的频域/幅值重平衡，缓解过平滑与细节缺失，使生成图更清晰、更有质感。  

**不足点：**  
- 评测很大一部分依赖“人类偏好投票/主观质量”，不同提示集合与不同模型版本上效果可能不完全一致；此外它主要解决细节与纹理问题，对复杂组合概念、精确可控等问题不是直接方案。  


---

### 📖 AGLLDiff: Attribute Guidance Diffusion for Real-world Low-Light Image Enhancement (arXiv 2024)

**数据集：**  
（配对低光增强）  
[LOL-v1（BMVC 2018 官方页）](https://daooshee.github.io/BMVC2018website/)  
> 用于：经典配对低光增强训练/测试基准（低光图 ↔ 正常曝光图）。  

[LOLv2（官方划分在论文/代码中给出，常见镜像可在社区数据页获取）](https://huggingface.co/datasets/real-stanford/LOLv2)  
> 用于：更大规模/更复杂的配对低光数据（含 synthetic/real 等设置，用于检验泛化）。  

[SICE](https://github.com/csjcai/SICE)  
> 用于：多曝光/多参考的低光增强基准之一，检验在不同曝光程度下的恢复质量。  

（真实无配对低光增强常用基准）  
[LIME](https://github.com/estija/LIME)  
> 用于：无参考/无配对的真实低光图像增强评测集之一。  

[NPE](https://github.com/baidut/BIMEF)  
> 用于：无配对真实低光增强评测集之一（常见聚合下载入口见 BIMEF）。  

[MEF](https://github.com/baidut/BIMEF)  
> 用于：多曝光融合/真实场景增强相关基准（常用于无参考指标对比）。  

[DICM](https://github.com/baidut/BIMEF)  
> 用于：真实低光/对比度不足场景评测集之一。  

[VV](https://github.com/baidut/BIMEF)  
> 用于：论文中使用的真实无配对评测集之一（常在聚合下载仓库提供）。  

**创新点：**  
- 以“图像属性（曝光、结构、颜色等）”作为引导信号，把低光增强转化为扩散采样过程中的属性引导优化，避免强依赖某一种固定退化模型假设。  
- 通过动态引导强度/梯度步数等机制，提高在极端低光下的可控性与稳定性。  

**不足点：**  
- 论文在结论中指出仍有改进空间：采样速度仍偏慢（需要加速采样技巧），以及可探索更丰富的高质量属性来进一步提升恢复效果与泛化。  

